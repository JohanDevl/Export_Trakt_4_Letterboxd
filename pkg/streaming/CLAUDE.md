# CLAUDE.md - Traitement Streaming MÃ©moire

## Module Overview

Ce module implÃ©mente un systÃ¨me de traitement streaming pour gÃ©rer efficacement de gros volumes de donnÃ©es avec une utilisation mÃ©moire minimale, processing par chunks et pipeline de transformation.

## Architecture du Module

### ðŸŒŠ Streaming Processor
```go
type StreamingProcessor struct {
    chunkSize    int
    bufferSize   int
    pipeline     []ProcessorStage
    errorHandler ErrorHandler
    metrics      *StreamingMetrics
}

type ProcessorStage interface {
    Process(chunk []interface{}) ([]interface{}, error)
    GetName() string
    IsParallel() bool
}

type Chunk struct {
    Data     []interface{}
    Index    int
    Metadata map[string]interface{}
}
```

### ðŸ“Š Avantages Performance

#### RÃ©duction MÃ©moire (80%)
- **Streaming** : Traitement par petits chunks au lieu de tout charger
- **Backpressure** : ContrÃ´le automatique du flux de donnÃ©es
- **Pipeline** : Transformation en Ã©tapes sans accumulation

#### Processing Efficace
- **ParallÃ©lisation** : Stages concurrents quand possible
- **Buffer Circulaire** : RÃ©utilisation de la mÃ©moire
- **Fault Tolerance** : Retry per-chunk en cas d'erreur

### ðŸ”„ Pipeline de Transformation

#### Stages de Processing
```go
// Stage 1: RÃ©cupÃ©ration API par chunks
type APIFetchStage struct {
    client    *api.Client
    chunkSize int
}

func (afs *APIFetchStage) Process(chunk []interface{}) ([]interface{}, error) {
    var results []interface{}
    
    for _, item := range chunk {
        if movie, ok := item.(MovieRequest); ok {
            movieData, err := afs.client.GetMovieDetails(movie.ID)
            if err != nil {
                continue // Skip erreurs individuelles
            }
            results = append(results, movieData)
        }
    }
    
    return results, nil
}

// Stage 2: Transformation format
type FormatTransformStage struct{}

func (fts *FormatTransformStage) Process(chunk []interface{}) ([]interface{}, error) {
    var transformed []interface{}
    
    for _, item := range chunk {
        if movie, ok := item.(api.Movie); ok {
            letterboxdMovie := transformToLetterboxdFormat(movie)
            transformed = append(transformed, letterboxdMovie)
        }
    }
    
    return transformed, nil
}

// Stage 3: Ã‰criture CSV streaming
type CSVWriteStage struct {
    writer *csv.Writer
}

func (cws *CSVWriteStage) Process(chunk []interface{}) ([]interface{}, error) {
    for _, item := range chunk {
        if movie, ok := item.(LetterboxdMovie); ok {
            row := movieToCSVRow(movie)
            if err := cws.writer.Write(row); err != nil {
                return nil, err
            }
        }
    }
    
    cws.writer.Flush()
    return chunk, nil // Pass-through pour monitoring
}
```

### ðŸš€ Configuration

#### Streaming Config
```toml
[streaming]
enabled = true
chunk_size = 1000          # Ã‰lÃ©ments par chunk
buffer_size = 8192         # Buffer I/O
max_concurrent_stages = 3   # ParallÃ©lisme pipeline
memory_limit = "100MB"     # Limite mÃ©moire

[streaming.backpressure]
enabled = true
threshold = 0.8            # Seuil dÃ©clenchement (80%)
strategy = "slow_down"     # slow_down, block, drop
```

### ðŸ“ˆ Utilisation

#### Stream Processing Simple
```go
processor := streaming.NewProcessor(streaming.Config{
    ChunkSize:  1000,
    BufferSize: 8192,
})

// Configuration pipeline
processor.AddStage(&APIFetchStage{client: traktClient})
processor.AddStage(&FormatTransformStage{})
processor.AddStage(&CSVWriteStage{writer: csvWriter})

// Traitement streaming
input := make(chan interface{}, 100)
output := make(chan interface{}, 100)

go processor.Process(input, output)

// Alimentation du stream
for _, movieID := range movieIDs {
    input <- MovieRequest{ID: movieID}
}
close(input)

// Collecte rÃ©sultats
for result := range output {
    // Traitement des rÃ©sultats transformÃ©s
}
```

Ce module permet le traitement efficace de datasets volumineux avec une empreinte mÃ©moire constante et optimisÃ©e.